---
layout: post
title:
date: '2021-11-25'
categories: DSA
note:
mathjax: true
mermaid: true
publish: true
---

## Introduction

Before we dive into it, please understand concept of [complexity]({{site.baseurl}}/dsa/2022/05/02/complexity.html). Data structures describe how data can be organized, stored, and manipulated. Some common data structures include arrays, linked lists, stacks, queues, trees, graphs, and hash tables. Algorithms are procedures for solving problems such as searching for information, sorting data, processing graphs, performing calculations, and many other tasks.

* Structure
  * array, arraylist, string
  * linked list
  * stack
  * queue
  * tree
  * graph
  * hash table
  * ...
* Most common algorithms (according to ChatGPT)
  * string
    * reversing a string
    * string is a palindrome
    * longest common prefix
  * array
    * finding the maximum subarray
    * binary search
    * sorting an array
      * merge sort
      * quick sort
  * dynamic programming
    * longest increasing subsequence
    * 0/1 knapsack problem
  * Graph traversal
    * shortest path between two nodes
    * detecting cycles in a graph
  * ...
* key concepts (according to cracking the coding interview)
  * bit Manipulation
  * memory (stack vs. heap)
  * recursion
  * dynamic programming
  * big O about time and space complexity

## Why

To solve problem in a more efficient way, making no more space to improve both the time complexity and space complexity.

## How

We can decompose all the programming logics into four actions:

* create (insert)
* read (access with id)
* update (access with attributes)
* delete (delete)

| (time complexity) | (arraylist) | (linkedlist) | (stack) | (queue) | (trees) |
| :--- | :----: | :----: | :----: | :----: |
| C | O(1) - O(n) | O(1) | O(1) | O(1) | O(n) |
| R | O(1) | O(n) | O(1) | O(1) | O(n) |
| U | O(1) | O(n) | O(1) | O(n) | O(1) |
| D | O(1) - O(n) | O(n) | O(1) | O(1) | O(log(n)) - O(n) |

### array

* example:

```javascript
[1, 2, 3, 4]
```

* create: O(1) to O(N)
  * inserting an element at the end of an array has $$O(1)$$
  * inserting an element at the beginning of an array has $$O(N)$$ because we need to shift all the elements
* read: O(1)
  * given an index i, the read operation in an ArrayList retrieves the element at that index in constant time, regardless of the size of the ArrayList.
  * linear search has $$O(N)$$ because it needs to search the value from beginning
* update: O(1)
  * Given an index i and a new value v, the update operation in an ArrayList replaces the element at index i with the new value v in constant time, regardless of the size of the ArrayList.
* delete: O(1) to O(N)
  * in the worst case, where n is the number of elements in the ArrayList because deleting an element requires shifting all subsequent elements one position to the left to fill the gap left by the deleted element. Deleting the last element in an ArrayList has a time complexity of O(1) on average because it can be done by simply updating the size of the ArrayList

### linkedlist

* singly linked list: pointer points the next node in each node
<div class="mermaid">
graph LR
  id1((A)) --> id2((B))
  id2((B)) --> id3((C))
  id3((C)) --> id4((...))
</div>
* doubly linked list: pointer points the next and previous nodes in each node
<div class="mermaid">
graph LR
  id1((A)) --> id2((B))
  id2((B)) --> id1((A))
  id2((B)) --> id3((C))
  id3((C)) --> id2((B))
  id3((C)) --> id4((...))
  id4((...)) --> id3((C))
</div>

* coding example:

```javascript
// Define a Node class for the elements of the linked list
class Node {
  constructor(data, next = null) {
    this.data = data;
    this.next = next;
  }
}

// Define the LinkedList class
class LinkedList {
  constructor() {
    this.head = null;
    this.tail = null;
    this.length = 0;
  }

  // Add a new element to the end of the list
  add(data) {
    const node = new Node(data);
    if (!this.head) {
      this.head = node;
      this.tail = node;
    } else {
      this.tail.next = node;
      this.tail = node;
    }
    this.length++;
  }

  // Get an element at a specified index
  get(index) {
    if (index < 0 || index >= this.length) {
      return null;
    }
    let current = this.head;
    for (let i = 0; i < index; i++) {
      current = current.next;
    }
    return current.data;
  }

  // Remove an element at a specified index
  remove(index) {
    if (index < 0 || index >= this.length) {
      return null;
    }
    let current = this.head;
    if (index === 0) {
      this.head = current.next;
      if (this.length === 1) {
        this.tail = null;
      }
    } else {
      let previous = null;
      for (let i = 0; i < index; i++) {
        previous = current;
        current = current.next;
      }
      previous.next = current.next;
      if (index === this.length - 1) {
        this.tail = previous;
      }
    }
    this.length--;
    return current.data;
  }
}

// Example usage
const list = new LinkedList();
list.add('a');
list.add('b');
list.add('c');
console.log(list.get(1)); // Output: "b"
list.remove(1);
console.log(list.get(1)); // Output: "c"
```

* create: O(1)
  * adding a new element to a LinkedList involves creating a new node and updating the head pointer to point to the new node, which can be done in constant time
* read: O(n)
  * In singly-linked list, the time complexity of read is O(n) in the worst case, where n is the length of the list Unlike an array, where elements are stored contiguously in memory and can be accessed in constant time using an index, in a linked list, we have to traverse the list from the head node to the desired index, which takes linear time proportional to the size of the list.
* update: O(n)
  * In a singly-linked list, the time complexity of update (i.e., modifying an element by index) is also O(n) in the worst case, where n is the length of the list. This is because like accessing an element, we need to traverse the list from the head node to the desired index to update it. Once we have reached the node, updating it takes constant time, i.e., O(1).
* delete: O(n)
  * In a singly-linked list, the time complexity of delete (i.e., removing an element by index) is also O(n) in the worst case, where n is the length of the list. This is because we need to traverse the list from the head node to the node immediately before the one we want to delete, which takes linear time proportional to the size of the list. Once we have found the node before the one we want to delete, we can remove the target node in constant time, i.e., O(1), by updating its predecessor's next pointer to skip over the target node.

### stack

<img src="{{site.baseurl}}/assets/img/stack.png" alt="">
image source: https://www.geeksforgeeks.org/stack-data-structure/

* coding example:

```javascript
class Stack {
  constructor() {
    this.items = [];
  }

  push(element) {
    this.items.push(element);
  }

  pop() {
    if (this.items.length === 0) {
      return "Underflow";
    }
    return this.items.pop();
  }

  peek() {
    return this.items[this.items.length - 1];
  }

  isEmpty() {
    return this.items.length === 0;
  }

  size() {
    return this.items.length;
  }

  print() {
    console.log(this.items.toString());
  }
}

// Example usage
const stack = new Stack();
stack.push(10);
stack.push(20);
stack.push(30);
console.log(stack.peek()); // Output: 30
console.log(stack.pop()); // Output: 30
stack.print(); // Output: [10, 20]
```

* create: O(1)
  * simply initializing a new stack data structure with an empty array or linked list, which can be done in constant time, regardless of the size of the stack.
* read: O(1)
  * we can only access the top, so the time complexity of this operation is O(1), which is constant time complexity.
* update: O(1)
  * Again, we can only access the top, so the time complexity of this operation is O(1)
* delete:
  * Again, we can only access the top, so the time complexity of this operation is O(1)

### queue

<img src="{{site.baseurl}}/assets/img/queue.png" alt="">
image source: https://www.geeksforgeeks.org/queue-data-structure/

* code example:

```javascript
class Queue {
  constructor() {
    this.items = [];
  }

  enqueue(element) {
    this.items.push(element);
  }

  dequeue() {
    if (this.isEmpty()) {
      return "Queue is empty";
    }
    return this.items.shift();
  }

  peek() {
    if (this.isEmpty()) {
      return "Queue is empty";
    }
    return this.items[0];
  }

  isEmpty() {
    return this.items.length === 0;
  }

  size() {
    return this.items.length;
  }

  print() {
    console.log(this.items.toString());
  }
}

// Example usage
const queue = new Queue();
console.log(queue.isEmpty()); // true
queue.enqueue(1);
queue.enqueue(2);
queue.enqueue(3);
queue.print(); // [1, 2, 3]
console.log(queue.size()); // 3
console.log(queue.peek()); // 1
console.log(queue.dequeue()); // 1
console.log(queue.print()); // [2, 3]
console.log(queue.size()); // 2
```

* create: O(1)
  * the enqueue method will concatenate item on the last without elements shifting
* read: O(1)
  * reading the front element of the queue, then the time complexity of this operation is O(1)
  * accessing elements from the middle or end of a queue is not supported, which can result in a time complexity of O(n).
* update: O(n)
  * dequeuing an element from the front of the queue may require shifting all the remaining elements one position to the front of the queue, which takes O(n) time in the worst case. Then, enqueuing the modified element again requires O(1) time complexity. Therefore, the overall time complexity for updating an element in a queue is O(n).
* delete: O(1)
  * deleting an element from the front of a queue simply involves removing the first element of an array-based queue or the head node of a linked-list based queue, which can be done in constant time, regardless of the size of the queue.

### trees

* code example (binary tree)

```javascript
class TreeNode {
  constructor(val = null, left = null, right = null) {
    this.val = val;
    this.left = left;
    this.right = right;
  }
}

function createTree(data) {
  if (!data.length) {  // Base case: empty data
    return null;
  }

  const mid = Math.floor(data.length / 2);  // Find middle index of data
  const root = new TreeNode(data[mid]);  // Create root node with middle value

  // Recursively create left and right subtrees
  root.left = createTree(data.slice(0, mid));
  root.right = createTree(data.slice(mid + 1));

  return root;
}
```

* create: O(n)
  * using a recursive algorithm is O(n), where n is the number of nodes in the tree. The recursive algorithm traverses each node in the tree exactly once. Specifically, at each level of the recursion, the algorithm creates one node and recursively calls itself twice to create the left and right subtrees. Since each node is created exactly once, the total number of operations performed by the algorithm is proportional to the number of nodes in the tree, which is O(n).
  * Inserting a new node into a binary tree requires finding the correct position for the new node in the tree. In the worst case, this involves traversing the height of the tree, which has a time complexity of O(log n) for a balanced binary tree and O(n) for an unbalanced binary tree.
* read: O(n)
  * Pre-order traversal visits each node in the tree in the order root, left subtree, right subtree. The time complexity of pre-order traversal is O(n), where n is the number of nodes in the tree.
  * In-order traversal visits each node in the tree in the order left subtree, root, right subtree. The time complexity of in-order traversal is also O(n), where n is the number of nodes in the tree.
  * Post-order traversal visits each node in the tree in the order left subtree, right subtree, root. The time complexity of post-order traversal is also O(n), where n is the number of nodes in the tree.
  * Level-order traversal visits each node in the tree by level, starting at the root and moving down to each level. The time complexity of level-order traversal is also O(n), where n is the number of nodes in the tree.
* update: O(1)
  * Updating a node in a binary tree involves finding the node to be updated and then modifying its value. This can be done in O(1) time once the node has been found (O(n)).
* delete: O(log n) to O(n)
  * Deleting a node from a binary tree requires finding the node to be deleted and then rearranging the tree to maintain its properties. In the worst case, this also involves traversing the height of the tree, which has a time complexity of O(log n) for a balanced binary tree and O(n) for an unbalanced binary tree.

### graphs

* code example

```javascript
class Graph {
  constructor() {
    this.nodes = new Map();
  }

  addNode(node) {
    this.nodes.set(node, []);
  }

  addEdge(nodeA, nodeB) {
    if (!this.nodes.has(nodeA)) {
      this.addNode(nodeA);
    }
    if (!this.nodes.has(nodeB)) {
      this.addNode(nodeB);
    }
    this.nodes.get(nodeA).push(nodeB);
    this.nodes.get(nodeB).push(nodeA);
  }

  getNeighbors(node) {
    return this.nodes.get(node);
  }
}

// Create a new graph
const graph = new Graph();

// Add nodes and edges to the graph
graph.addEdge("A", "B");
graph.addEdge("B", "C");
graph.addEdge("C", "D");
graph.addEdge("D", "E");
graph.addEdge("E", "F");
graph.addEdge("F", "G");
graph.addEdge("G", "A");

// Get the neighbors of node "C"
const neighborsOfC = graph.getNeighbors("C");
console.log(neighborsOfC); // Output: ["B", "D"]
```

* time complexity (to be continued)

### tries

to be continued

### hash table

The data flow:

<div class="mermaid">
graph LR
  id1(value 1) --> id4(hash<br>function)
  id2(value 2) --> id4(hash<br>function)
  id3(value 3) --> id4(hash<br>function)

  id4(hash<br>function) -- insert value 1 --> id5(key 0)
  id4(hash<br>function) -- insert value 2 --> id7(key 1)
  id4(hash<br>function) -- insert value 3 --> id7(key 2)
  subgraph Buckets
    id5(key 0)
    id6(key 1)
    id7(key 2)
    id8(key 3)
    ...
  end

  id5(key 0) --> id9(value 1)
  id7(key 2) --> id10(value 2)
  id10(value 2) --> id11(value 3)
</div>

The value will be calculated by certain method in hash function to get the key of buckets and then connects the value with linked list if the mapping key is the same.

Given the length of buckets is N, meaning there are N keys, if we want to search an element, it will first pass the element into hash function to get the key for certain linked list and then search through the linked list.

The time complexity = $$O(A + B)$$, where A is the length of bucket and B is the length of linked list. A is actually 1 because the magic of hash function and B is usually close to 1 if we making the collisions, meaning values map to same key, as low as possible. As a result, the time complexity is actually $$O(1)$$

### dynamic programming

When we need to solve the optimal subproblem in the substructure of the whole structure. The optimal solution can be composed of optimal solutions to its subproblems.

A tyical example: Maximum subarray sum

Please return the sub array that has biggest sum

```javascript
function maxSubarraySum(arr) {
  const dp = [];
  dp[0] = arr[0];
  let maxSum = dp[0];
  
  for (let i = 1; i < arr.length; i++) {
    dp[i] = Math.max(arr[i], dp[i - 1] + arr[i]);
    maxSum = Math.max(maxSum, dp[i]);
  }
  
  return maxSum;
}

// Example usage:
const array = [1, -3, 2, 1, -1];
const result = maxSubarraySum(array); // Returns 3
```

By storing the solutions to the subproblems, dynamic programming can avoid redundant calculations and improve the overall efficiency of the algorithm.

## what

Given we have the knowledge above and we know [how to solve problem]({{site.baseurl}}//dsa/2023/03/08/solve-problem.html).

### array example

[array]({{site.baseurl}}/dsa/2022/05/22/array.html)

### linkedlist example

[linkedlist]({{site.baseurl}}/dsa/2022/05/23/linked_list.html)

## reference

cracking the coding interview

[**The top data structures you should know for your next coding interview**](https://www.freecodecamp.org/news/the-top-data-structures-you-should-know-for-your-next-coding-interview-36af0831f5e3/)
